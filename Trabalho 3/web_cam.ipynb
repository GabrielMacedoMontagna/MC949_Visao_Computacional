{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_markdown",
   "metadata": {},
   "source": [
    "## Navegação Assistida por Visão Computacional (T3)\n",
    "\n",
    "Este notebook implementa o pipeline principal para um sistema de navegação assistida, utilizando:\n",
    "- **Detecção de Objetos:** YOLOv8n com tracking (ByteTrack).\n",
    "- **Estimativa de Profundidade:** MiDaS (Small) para profundidade monocular relativa.\n",
    "- **Processamento Temporal:** Filtro de Média Móvel Simples (SMA) para suavizar a profundidade.\n",
    "- **Estimativa Métrica:** Conversão da profundidade relativa para metros (requer calibração).\n",
    "- **Lógica de Navegação:** Geração de comandos direcionais básicos.\n",
    "- **Feedback Auditivo:** Text-to-Speech (TTS) com rate limiting e detalhes do objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports_cleaned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas Padrão\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import threading\n",
    "from typing import List, Tuple\n",
    "from collections import deque # Usado no filtro SMA\n",
    "\n",
    "# Bibliotecas de Terceiros\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from gtts import gTTS\n",
    "\n",
    "# Configuração para display de áudio\n",
    "try:\n",
    "    from IPython.display import Audio, display\n",
    "    # Tenta importar playsound para reprodução fora do Jupyter\n",
    "    import playsound\n",
    "    USE_PLAYSOUND = True\n",
    "except ImportError:\n",
    "    USE_PLAYSOUND = False\n",
    "    print(\"[Aviso] Biblioteca 'playsound' não encontrada. O áudio será apenas salvo ou tentará usar IPython.display.\")\n",
    "    print(\"         Para tocar o áudio fora do Jupyter, instale: pip install playsound==1.2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_loading_markdown",
   "metadata": {},
   "source": [
    "### Carregamento dos Modelos (MiDaS e YOLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_models_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo MiDaS: MiDaS_small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Pichau/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "c:\\Users\\Pichau\\miniconda3\\envs\\visao_computacional\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Pichau/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiDaS carregado no dispositivo: cuda\n",
      "Carregando modelo YOLO: yolov8n.pt...\n",
      "Modelo YOLO carregado com sucesso.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Pichau/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 640x640 (no detections), 6.4ms\n",
      "Speed: 7.3ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Modelo YOLO inicializado.\n"
     ]
    }
   ],
   "source": [
    "def load_midas_model(model_type: str = \"MiDaS_small\"):\n",
    "    \"\"\"Carrega o modelo MiDaS especificado e a transformação correspondente.\"\"\"\n",
    "    print(f\"Carregando modelo MiDaS: {model_type}...\")\n",
    "    # Força recarregar para evitar possíveis problemas de cache do torch.hub\n",
    "    # torch.hub.download_url_to_file('https://github.com/intel-isl/MiDaS/archive/master.zip', 'midas.zip')\n",
    "    # !unzip -o midas.zip\n",
    "    # midas = torch.hub.load(\"MiDaS-master\", model_type, source='local', trust_repo=True) \n",
    "    midas = torch.hub.load(\"intel-isl/MiDaS\", model_type, trust_repo=True)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    midas.to(device)\n",
    "    midas.eval()\n",
    "\n",
    "    midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
    "    # O transform 'small' é compatível com DPT_Hybrid e MiDaS_small\n",
    "    transform = midas_transforms.small_transform\n",
    "    print(f\"MiDaS carregado no dispositivo: {device}\")\n",
    "    return midas, transform, device\n",
    "\n",
    "def load_yolo_model(model_name: str = 'yolov8n.pt'):\n",
    "    \"\"\"Carrega o modelo YOLO especificado.\"\"\"\n",
    "    print(f\"Carregando modelo YOLO: {model_name}...\")\n",
    "    try:\n",
    "        model = YOLO(model_name)\n",
    "        print(\"Modelo YOLO carregado com sucesso.\")\n",
    "        # Força uma inferência inicial para otimizar o tempo no primeiro frame\n",
    "        _ = model(np.zeros((640, 640, 3), dtype=np.uint8))\n",
    "        print(\"Modelo YOLO inicializado.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o modelo YOLO: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Carrega os modelos uma única vez\n",
    "midas_model, midas_transform, processing_device = load_midas_model(\"MiDaS_small\")\n",
    "yolo_model = load_yolo_model('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_functions_markdown",
   "metadata": {},
   "source": [
    "### Funções Auxiliares (Processamento e Lógica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helper_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(midas, input_batch, img_shape):\n",
    "    \"\"\"Executa a inferência do MiDaS e retorna o mapa de profundidade normalizado.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=img_shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "    output = prediction.cpu().numpy()\n",
    "    output_min, output_max = output.min(), output.max()\n",
    "    if output_max > output_min:\n",
    "        output = (output - output_min) / (output_max - output_min)\n",
    "    else:\n",
    "        output = np.zeros_like(output)\n",
    "    return output\n",
    "\n",
    "def is_center_in_roi(bbox: tuple[int, int, int, int], roi: tuple[int, int, int, int] | None):\n",
    "    \"\"\"Verifica se o centro da BBox está dentro da ROI.\"\"\"\n",
    "    if roi is None: return True\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi_x1, roi_y1, roi_x2, roi_y2 = roi\n",
    "    obj_center_x = (x1 + x2) / 2\n",
    "    obj_center_y = (y1 + y2) / 2\n",
    "    return (roi_x1 <= obj_center_x <= roi_x2) and (roi_y1 <= obj_center_y <= roi_y2)\n",
    "\n",
    "def get_horizontal_position(bbox: tuple[int, int, int, int], image_width: int):\n",
    "    \"\"\"Determina a posição horizontal (Esquerda/Centro/Direita) baseado em terços.\"\"\"\n",
    "    x1, _, x2, _ = bbox\n",
    "    obj_center_x = (x1 + x2) / 2\n",
    "    limite_esquerda = image_width / 3\n",
    "    limite_direita = (image_width * 2) / 3\n",
    "    if obj_center_x < limite_esquerda: return \"Esquerda\"\n",
    "    elif obj_center_x <= limite_direita: return \"Centro\"\n",
    "    else: return \"Direita\"\n",
    "\n",
    "def get_object_depth_dict(yolo_results, depth_frame, img_shape, depth_mode: str = 'median', classes_alvo: list[str] | None = None, roi: tuple[int, int, int, int] | None = None):\n",
    "    \"\"\"\n",
    "    Combina detecções YOLO com mapa de profundidade, calcula profundidade relativa e posição.\n",
    "    Retorna lista de dicionários por objeto, incluindo 'track_id'.\n",
    "    \"\"\"\n",
    "    if classes_alvo is None:\n",
    "        classes_alvo = ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'bench', 'chair', 'stop sign', 'traffic light']\n",
    "\n",
    "    MODOS_DE_PROFUNDIDADE_VALIDOS = {'median', 'mean'}\n",
    "    if depth_mode not in MODOS_DE_PROFUNDIDADE_VALIDOS:\n",
    "        raise ValueError(f\"Modo de profundidade inválido: {depth_mode}\")\n",
    "\n",
    "    detected_objects = []\n",
    "    nomes_classes = yolo_results.names\n",
    "    image_height, image_width = img_shape[:2]\n",
    "    boxes = yolo_results.boxes\n",
    "\n",
    "    if boxes is None or len(boxes) == 0: return []\n",
    "    \n",
    "    # Garante que temos IDs de tracking para associar\n",
    "    has_track_ids = boxes.id is not None and len(boxes.id) > 0\n",
    "    track_ids = boxes.id.int().cpu().tolist() if has_track_ids else [-1] * len(boxes)\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        if box.cls is None or len(box.cls) == 0: continue\n",
    "        classe_id = int(box.cls[0])\n",
    "        nome = nomes_classes[classe_id]\n",
    "\n",
    "        if nome in classes_alvo:\n",
    "            if box.xyxy is None or len(box.xyxy) == 0: continue\n",
    "            coord_box = box.xyxy[0].int().cpu().numpy()\n",
    "\n",
    "            if is_center_in_roi(coord_box, roi):\n",
    "                x1, y1, x2, y2 = coord_box\n",
    "                y1, y2 = max(0, y1), min(image_height, y2)\n",
    "                x1, x2 = max(0, x1), min(image_width, x2)\n",
    "                if y1 >= y2 or x1 >= x2: continue\n",
    "\n",
    "                profundidades_box = depth_frame[y1:y2, x1:x2]\n",
    "                if profundidades_box.size == 0: continue\n",
    "\n",
    "                obj_depth_rel = float(np.median(profundidades_box)) if depth_mode == 'median' else float(np.mean(profundidades_box))\n",
    "                posicao_horizontal = get_horizontal_position(coord_box, image_width)\n",
    "                current_track_id = track_ids[i]\n",
    "\n",
    "                obj_data = {\n",
    "                    'nome': nome,\n",
    "                    'profundidade_rel': obj_depth_rel,\n",
    "                    'bbox': [int(c) for c in coord_box],\n",
    "                    'posicao': posicao_horizontal,\n",
    "                    'track_id': current_track_id\n",
    "                }\n",
    "                detected_objects.append(obj_data)\n",
    "\n",
    "    # Não reordena aqui, a ordenação será feita após o filtro temporal\n",
    "    return detected_objects\n",
    "\n",
    "def metric_proximity_label(distance_m):\n",
    "    \"\"\"Classifica a distância em metros em faixas de proximidade.\"\"\"\n",
    "    if distance_m < 1.0: return \"Próximo\"\n",
    "    elif distance_m <= 2.5: return \"Médio\"\n",
    "    else: return \"Longe\"\n",
    "\n",
    "def gerar_comando_navegacao(objetos_filtrados: list) -> Tuple[str, dict | None]:\n",
    "    \"\"\"\n",
    "    Gera comando de navegação baseado no objeto filtrado mais próximo.\n",
    "    Retorna o comando e os dados do objeto que o gerou (ou None).\n",
    "    \"\"\"\n",
    "    if not objetos_filtrados: return \"Siga em frente.\", None\n",
    "    \n",
    "    obj_mais_proximo = objetos_filtrados[0]\n",
    "    dist_m = obj_mais_proximo.get('distancia_m', 999)\n",
    "    pos = obj_mais_proximo['posicao']\n",
    "    prox_label = metric_proximity_label(dist_m)\n",
    "\n",
    "    comando = \"Siga em frente.\"\n",
    "    objeto_causador = None\n",
    "\n",
    "    if prox_label == \"Próximo\":\n",
    "        if pos == \"Centro\": comando = \"Pare!\"\n",
    "        elif pos == \"Esquerda\": comando = \"Desvie à Direita.\"\n",
    "        elif pos == \"Direita\": comando = \"Desvie à Esquerda.\"\n",
    "        objeto_causador = obj_mais_proximo # Associa o objeto ao comando de alerta\n",
    "        \n",
    "    return comando, objeto_causador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realtime_video_markdown",
   "metadata": {},
   "source": [
    "### Pipeline Principal: Captura e Processamento em Tempo Real\n",
    "\n",
    "Integra todas as funcionalidades: captura de vídeo, MiDaS, YOLO com tracking, filtro SMA, conversão métrica, lógica de navegação e TTS com rate limiting.\n",
    "\n",
    "**Instruções de Uso:**\n",
    "- Execute a célula abaixo para iniciar a câmera e o processamento\n",
    "- Para parar o sistema:\n",
    "  1. **Clique em uma das janelas OpenCV** (`Camera Feed`, `Tracked Feed`, ou `Depth Map`)\n",
    "  2. **Pressione a tecla 'q'** no seu teclado\n",
    "- O sistema fornecerá feedback auditivo quando detectar obstáculos próximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realtime_video_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câmera inicializada. Pressione 'q' para sair.\n",
      "TTS Gerado: 'Pare! bench a 0.6 metros no centro.'\n",
      "TTS Gerado: 'Pare! bench a 0.4 metros no centro.'\n",
      "TTS Gerado: 'Desvie à Direita. person a 0.5 metros no esquerda.'\n",
      "TTS Gerado: 'Pare! person a 0.4 metros no centro.'\n",
      "TTS Gerado: 'Desvie à Direita. car a 0.3 metros no esquerda.'\n",
      "TTS Gerado: 'Pare! person a 0.8 metros no centro.'\n",
      "TTS Gerado: 'Desvie à Esquerda. person a 0.4 metros no direita.'\n",
      "TTS Gerado: 'Desvie à Esquerda. person a 0.4 metros no direita.'\n",
      "TTS Gerado: 'Pare! traffic light a 0.4 metros no centro.'\n",
      "TTS Gerado: 'Desvie à Esquerda. chair a 0.6 metros no direita.'\n",
      "TTS Gerado: 'Desvie à Esquerda. chair a 0.4 metros no direita.'\n",
      "TTS Gerado: 'Desvie à Esquerda. chair a 0.8 metros no direita.'\n",
      "TTS Gerado: 'Pare! person a 0.5 metros no centro.'\n",
      "TTS Gerado: 'Pare! person a 0.7 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.6 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.5 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.5 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.5 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.5 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.7 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.9 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.7 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.7 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.7 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.8 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.6 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.5 metros no centro.'\n",
      "TTS Gerado: 'Pare! person a 0.5 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.7 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.8 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.9 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 1.0 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 1.0 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.9 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 1.0 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.4 metros no centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.4 metros no centro.'\n",
      "TTS Gerado: 'Pare! person a 0.4 metros no centro.'\n",
      "TTS Gerado: 'Pare! person a 0.6 metros no centro.'\n",
      "TTS Gerado: 'Desvie à Esquerda. person a 0.5 metros no direita.'\n",
      "TTS Gerado: 'Pare! chair a 0.6 metros no centro.'\n",
      "TTS Gerado: 'Pare! bus a 0.7 metros no centro.'\n",
      "TTS Gerado: 'Pare! person a 0.5 metros no centro.'\n",
      "Captura de vídeo encerrada e recursos liberados.\n"
     ]
    }
   ],
   "source": [
    "# Imports para TTS e áudio\n",
    "from gtts import gTTS\n",
    "import tempfile\n",
    "import os\n",
    "import threading # Para tocar áudio sem bloquear o loop principal\n",
    "\n",
    "# Inicializar a captura de vídeo\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Erro: Não foi possível abrir a câmera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Câmera inicializada. Pressione 'q' para sair.\")\n",
    "\n",
    "# -- Configurações e Variáveis de Estado --\n",
    "prev_frame_time = time.time()\n",
    "last_tts_time = 0.0\n",
    "TTS_RATE_LIMIT_SECONDS = 3.0\n",
    "SMA_WINDOW_SIZE = 10\n",
    "depth_history = {}\n",
    "last_seen_frame = {}\n",
    "frame_count = 0\n",
    "fps_list = deque(maxlen=20)\n",
    "\n",
    "# -- Constante de Calibração --\n",
    "# 1. Coloque um objeto a uma distância conhecida (ex: 1.0 metro).\n",
    "# 2. Rode o código e observe a profundidade RELATIVA FILTRADA (filtered_rel_depth) para o track_id do objeto.\n",
    "# C = distancia_real_metros * filtered_rel_depth.\n",
    "CALIBRATION_CONSTANT_C = 0.3 \n",
    "\n",
    "def relative_to_metric(relative_depth, calibration_constant):\n",
    "    \"\"\"Converte profundidade relativa (0-1) para estimativa de distância em metros.\"\"\"\n",
    "    return calibration_constant / (relative_depth + 1e-6)\n",
    "\n",
    "def play_audio_async(file_path):\n",
    "    \"\"\"Toca um arquivo de áudio em uma thread separada e o remove depois.\"\"\"\n",
    "    def target():\n",
    "        try:\n",
    "            if USE_PLAYSOUND:\n",
    "                playsound.playsound(file_path)\n",
    "            else:\n",
    "                # No Jupyter, display pode funcionar, mas fora dele não.\n",
    "                print(f\"(Simulando áudio - playsound não disponível): Tocar {file_path}\")\n",
    "                # display(Audio(file_path, autoplay=True)) # Descomente se estiver no Jupyter\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao tocar áudio '{file_path}': {e}\")\n",
    "        finally:\n",
    "            time.sleep(0.5) # Pequena pausa antes de tentar remover\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao remover arquivo de áudio temporário '{file_path}': {e}\")\n",
    "                \n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.daemon = True # Permite que o programa saia mesmo se a thread de áudio estiver ativa\n",
    "    thread.start()\n",
    "\n",
    "# --- Loop Principal ---\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        new_frame_time = time.time()\n",
    "        if not ret: print(\"Erro: Não foi possível ler o frame.\"); break\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        input_batch = midas_transform(frame_rgb).to(processing_device)\n",
    "        output_depth = get_depth(midas_model, input_batch, frame.shape)\n",
    "        objects_result = yolo_model.track(frame_rgb, persist=True, tracker='bytetrack.yaml', verbose=False)\n",
    "\n",
    "        altura, largura = frame.shape[:2]\n",
    "        roi = (largura // 6, 0, (largura * 5) // 6, altura)\n",
    "        \n",
    "        # Obtém objetos detectados com profundidade relativa e track_id\n",
    "        objetos_detectados = get_object_depth_dict(objects_result[0], output_depth, frame.shape, depth_mode='median', roi=roi)\n",
    "\n",
    "        # -- Filtro Temporal e Conversão Métrica --\n",
    "        frame_count += 1\n",
    "        filtered_relative_depths = {}\n",
    "        metric_depths = {}\n",
    "        current_track_ids = set()\n",
    "\n",
    "        for obj in objetos_detectados:\n",
    "            track_id = obj.get('track_id', -1)\n",
    "            if track_id != -1:\n",
    "                current_track_ids.add(track_id)\n",
    "                last_seen_frame[track_id] = frame_count\n",
    "                current_rel_depth = obj['profundidade_rel']\n",
    "                \n",
    "                history = depth_history.setdefault(track_id, deque(maxlen=SMA_WINDOW_SIZE))\n",
    "                history.append(current_rel_depth)\n",
    "                \n",
    "                filtered_rel_depth = float(np.mean(history))\n",
    "                filtered_relative_depths[track_id] = filtered_rel_depth\n",
    "                metric_depths[track_id] = relative_to_metric(filtered_rel_depth, CALIBRATION_CONSTANT_C)\n",
    "\n",
    "        # -- Limpeza de IDs Antigos --\n",
    "        if frame_count % 10 == 0:\n",
    "            ids_to_remove = [tid for tid, last_seen in last_seen_frame.items() if frame_count - last_seen > SMA_WINDOW_SIZE * 3] # Aumentado limiar de limpeza\n",
    "            for tid in ids_to_remove:\n",
    "                depth_history.pop(tid, None)\n",
    "                last_seen_frame.pop(tid, None)\n",
    "\n",
    "        # -- Preparar Lista Final de Objetos para Navegação --\n",
    "        objetos_para_navegacao = []\n",
    "        for obj in objetos_detectados:\n",
    "            track_id = obj.get('track_id', -1)\n",
    "            if track_id != -1 and track_id in metric_depths:\n",
    "                obj_final = obj.copy()\n",
    "                obj_final['distancia_m'] = metric_depths[track_id]\n",
    "                obj_final.pop('profundidade_rel', None)\n",
    "                objetos_para_navegacao.append(obj_final)\n",
    "            elif track_id == -1: # Fallback para objetos sem tracking\n",
    "                 obj_final = obj.copy()\n",
    "                 obj_final['distancia_m'] = relative_to_metric(obj['profundidade_rel'], CALIBRATION_CONSTANT_C)\n",
    "                 obj_final.pop('profundidade_rel', None)\n",
    "                 objetos_para_navegacao.append(obj_final)\n",
    "\n",
    "        objetos_para_navegacao.sort(key=lambda x: x.get('distancia_m', 999))\n",
    "\n",
    "        # -- Geração de Comando e Feedback --\n",
    "        comando, objeto_causador = gerar_comando_navegacao(objetos_para_navegacao)\n",
    "        elapsed_time = new_frame_time - prev_frame_time\n",
    "        fps_list.append(elapsed_time)\n",
    "        avg_elapsed_time = np.mean(fps_list)\n",
    "        fps = 1.0 / avg_elapsed_time if avg_elapsed_time > 0 else 0\n",
    "        prev_frame_time = new_frame_time\n",
    "        #print(f\"FPS: {fps:.1f} | Comando: {comando}\") # Movido para exibição no frame\n",
    "\n",
    "        # -- Visualização --\n",
    "        depth_visual = (output_depth * 255).astype(np.uint8)\n",
    "        depth_colored = cv2.applyColorMap(depth_visual, cv2.COLORMAP_JET)\n",
    "        frame_tracked_plot = objects_result[0].plot() \n",
    "        frame_tracked_bgr = cv2.cvtColor(frame_tracked_plot, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        cv2.rectangle(frame, roi[:2], roi[2:], (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame_tracked_bgr, roi[:2], roi[2:], (0, 255, 0), 2)\n",
    "        fps_text = f\"FPS: {fps:.1f}\"\n",
    "        distancia_text = \"--\" \n",
    "        if objeto_causador: # Usa o objeto que causou o comando\n",
    "            dist_m = objeto_causador.get('distancia_m', 0)\n",
    "            distancia_text = f\"Dist: {dist_m:.2f}m ({objeto_causador['nome']})\"\n",
    "        elif objetos_para_navegacao: # Se comando é 'Siga', mostra o mais próximo\n",
    "            dist_m = objetos_para_navegacao[0].get('distancia_m', 0)\n",
    "            distancia_text = f\"Dist: {dist_m:.2f}m ({objetos_para_navegacao[0]['nome']})\"\n",
    "            \n",
    "        cv2.putText(frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, comando, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, distancia_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_tracked_bgr, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_tracked_bgr, comando, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_tracked_bgr, distancia_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"Camera Feed\", frame)\n",
    "        cv2.imshow(\"Tracked Feed\", frame_tracked_bgr)\n",
    "        cv2.imshow(\"Depth Map\", depth_colored)\n",
    "\n",
    "        # -- Lógica TTS com Rate Limiting e Detalhes --\n",
    "        current_time = time.time()\n",
    "        if comando != \"Siga em frente.\" and (current_time - last_tts_time >= TTS_RATE_LIMIT_SECONDS):\n",
    "            sentence = comando # Começa com o comando principal\n",
    "            if objeto_causador:\n",
    "                nome = objeto_causador['nome']\n",
    "                dist_m = objeto_causador['distancia_m']\n",
    "                pos = objeto_causador['posicao'].lower()\n",
    "                # Monta frase mais detalhada para o alerta\n",
    "                sentence = f\"{comando} {nome} a {dist_m:.1f} metros no {pos}.\"\n",
    "            \n",
    "            print(f\"TTS Gerado: '{sentence}'\") # Log do que será falado\n",
    "            try:\n",
    "                tts = gTTS(sentence, lang='pt')\n",
    "                # Usar um gerenciador de contexto para garantir fechamento\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmpf:\n",
    "                    temp_filename = tmpf.name\n",
    "                tts.save(temp_filename)\n",
    "                last_tts_time = current_time # Atualiza o timestamp\n",
    "                play_audio_async(temp_filename) # Toca em thread separada\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao gerar ou tocar TTS: {e}\")\n",
    "\n",
    "        # -- Saída --\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # --- Finalização Segura --- \n",
    "    # Garante que a câmera seja liberada e as janelas fechadas mesmo se ocorrer um erro no loop\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Captura de vídeo encerrada e recursos liberados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tts_description_markdown",
   "metadata": {},
   "source": [
    "### Funções de Classificação de Proximidade e Geração de Áudio (TTS)\n",
    "\n",
    "Define as faixas de distância métrica e a lógica para gerar os comandos de voz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_proximity_label(distance_m):\n",
    "    \"\"\"\n",
    "    Classifica a distância em metros em faixas de proximidade.\n",
    "    \n",
    "    Args:\n",
    "        distance_m (float): Distância estimada em metros.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'Próximo' (< 1m), 'Médio' (1-2.5m), ou 'Longe' (> 2.5m).\n",
    "    \"\"\"\n",
    "    if distance_m < 1.0:\n",
    "        return \"Próximo\"\n",
    "    elif distance_m <= 2.5:\n",
    "        return \"Médio\"\n",
    "    else:\n",
    "        return \"Longe\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visao_computacional",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
