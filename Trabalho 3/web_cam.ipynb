{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_markdown",
   "metadata": {},
   "source": [
    "## Navegação Assistida por Visão Computacional (T3)\n",
    "\n",
    "Este notebook implementa o pipeline principal para um sistema de navegação assistida, utilizando:\n",
    "- **Detecção de Objetos:** YOLOv8n com tracking (ByteTrack).\n",
    "- **Estimativa de Profundidade:** MiDaS (Small) para profundidade monocular relativa.\n",
    "- **Processamento Temporal:** Filtro de Média Móvel Simples (SMA) para suavizar a profundidade.\n",
    "- **Estimativa Métrica:** Conversão da profundidade relativa para metros (requer calibração).\n",
    "- **Lógica de Navegação:** Geração de comandos direcionais básicos.\n",
    "- **Feedback Auditivo:** Text-to-Speech (TTS) com rate limiting e detalhes do objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports_cleaned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas Padrão\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import threading\n",
    "from typing import List, Tuple\n",
    "from collections import deque # Usado no filtro SMA\n",
    "\n",
    "# Bibliotecas de Terceiros\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from gtts import gTTS\n",
    "\n",
    "# Configuração para display de áudio\n",
    "try:\n",
    "    from IPython.display import Audio, display\n",
    "    # Tenta importar playsound para reprodução fora do Jupyter\n",
    "    import playsound\n",
    "    USE_PLAYSOUND = True\n",
    "except ImportError:\n",
    "    USE_PLAYSOUND = False\n",
    "    print(\"[Aviso] Biblioteca 'playsound' não encontrada. O áudio será apenas salvo ou tentará usar IPython.display.\")\n",
    "    print(\"         Para tocar o áudio fora do Jupyter, instale: pip install playsound==1.2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_loading_markdown",
   "metadata": {},
   "source": [
    "### Carregamento dos Modelos (MiDaS e YOLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_models_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo MiDaS: MiDaS_small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\gabri/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "c:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\gabri/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n",
      "Using cache found in C:\\Users\\gabri/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiDaS carregado no dispositivo: cpu\n",
      "Carregando modelo YOLO: yolov8n.pt...\n",
      "Modelo YOLO carregado com sucesso.\n",
      "\n",
      "0: 640x640 (no detections), 507.6ms\n",
      "Speed: 16.4ms preprocess, 507.6ms inference, 18.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Modelo YOLO inicializado.\n"
     ]
    }
   ],
   "source": [
    "def load_midas_model(model_type: str = \"MiDaS_small\"):\n",
    "    \"\"\"Carrega o modelo MiDaS especificado e a transformação correspondente.\"\"\"\n",
    "    print(f\"Carregando modelo MiDaS: {model_type}...\")\n",
    "    # Força recarregar para evitar possíveis problemas de cache do torch.hub\n",
    "    # torch.hub.download_url_to_file('https://github.com/intel-isl/MiDaS/archive/master.zip', 'midas.zip')\n",
    "    # !unzip -o midas.zip\n",
    "    # midas = torch.hub.load(\"MiDaS-master\", model_type, source='local', trust_repo=True) \n",
    "    midas = torch.hub.load(\"intel-isl/MiDaS\", model_type, trust_repo=True)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    midas.to(device)\n",
    "    midas.eval()\n",
    "\n",
    "    midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
    "    # O transform 'small' é compatível com DPT_Hybrid e MiDaS_small\n",
    "    transform = midas_transforms.small_transform\n",
    "    print(f\"MiDaS carregado no dispositivo: {device}\")\n",
    "    return midas, transform, device\n",
    "\n",
    "def load_yolo_model(model_name: str = 'yolov8n.pt'):\n",
    "    \"\"\"Carrega o modelo YOLO especificado.\"\"\"\n",
    "    print(f\"Carregando modelo YOLO: {model_name}...\")\n",
    "    try:\n",
    "        model = YOLO(model_name)\n",
    "        print(\"Modelo YOLO carregado com sucesso.\")\n",
    "        # Força uma inferência inicial para otimizar o tempo no primeiro frame\n",
    "        _ = model(np.zeros((640, 640, 3), dtype=np.uint8))\n",
    "        print(\"Modelo YOLO inicializado.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o modelo YOLO: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Carrega os modelos uma única vez\n",
    "midas_model, midas_transform, processing_device = load_midas_model(\"MiDaS_small\")\n",
    "yolo_model = load_yolo_model('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_functions_markdown",
   "metadata": {},
   "source": [
    "### Funções Auxiliares (Processamento e Lógica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helper_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(midas, input_batch, img_shape):\n",
    "    \"\"\"Executa a inferência do MiDaS e retorna o mapa de profundidade normalizado.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=img_shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "    output = prediction.cpu().numpy()\n",
    "    output_min, output_max = output.min(), output.max()\n",
    "    if output_max > output_min:\n",
    "        output = (output - output_min) / (output_max - output_min)\n",
    "    else:\n",
    "        output = np.zeros_like(output)\n",
    "    return output\n",
    "\n",
    "def is_center_in_roi(bbox: tuple[int, int, int, int], roi: tuple[int, int, int, int] | None):\n",
    "    \"\"\"Verifica se o centro da BBox está dentro da ROI.\"\"\"\n",
    "    if roi is None: return True\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    roi_x1, roi_y1, roi_x2, roi_y2 = roi\n",
    "    obj_center_x = (x1 + x2) / 2\n",
    "    obj_center_y = (y1 + y2) / 2\n",
    "    return (roi_x1 <= obj_center_x <= roi_x2) and (roi_y1 <= obj_center_y <= roi_y2)\n",
    "\n",
    "def get_horizontal_position(bbox: tuple[int, int, int, int], image_width: int):\n",
    "    \"\"\"Determina a posição horizontal (Esquerda/Centro/Direita) baseado em terços.\"\"\"\n",
    "    x1, _, x2, _ = bbox\n",
    "    obj_center_x = (x1 + x2) / 2\n",
    "    limite_esquerda = image_width / 3\n",
    "    limite_direita = (image_width * 2) / 3\n",
    "    if obj_center_x < limite_esquerda: return \"Esquerda\"\n",
    "    elif obj_center_x <= limite_direita: return \"Centro\"\n",
    "    else: return \"Direita\"\n",
    "\n",
    "def get_object_depth_dict(yolo_results, depth_frame, img_shape, depth_mode: str = 'median', classes_alvo: list[str] | None = None, roi: tuple[int, int, int, int] | None = None):\n",
    "    \"\"\"\n",
    "    Combina detecções YOLO com mapa de profundidade, calcula profundidade relativa e posição.\n",
    "    Retorna lista de dicionários por objeto, incluindo 'track_id'.\n",
    "    \"\"\"\n",
    "    if classes_alvo is None:\n",
    "        classes_alvo = ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'bench', 'chair', 'stop sign', 'traffic light']\n",
    "\n",
    "    MODOS_DE_PROFUNDIDADE_VALIDOS = {'median', 'mean'}\n",
    "    if depth_mode not in MODOS_DE_PROFUNDIDADE_VALIDOS:\n",
    "        raise ValueError(f\"Modo de profundidade inválido: {depth_mode}\")\n",
    "\n",
    "    detected_objects = []\n",
    "    nomes_classes = yolo_results.names\n",
    "    image_height, image_width = img_shape[:2]\n",
    "    boxes = yolo_results.boxes\n",
    "\n",
    "    if boxes is None or len(boxes) == 0: return []\n",
    "    \n",
    "    # Garante que temos IDs de tracking para associar\n",
    "    has_track_ids = boxes.id is not None and len(boxes.id) > 0\n",
    "    track_ids = boxes.id.int().cpu().tolist() if has_track_ids else [-1] * len(boxes)\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        if box.cls is None or len(box.cls) == 0: continue\n",
    "        classe_id = int(box.cls[0])\n",
    "        nome = nomes_classes[classe_id]\n",
    "\n",
    "        if nome in classes_alvo:\n",
    "            if box.xyxy is None or len(box.xyxy) == 0: continue\n",
    "            coord_box = box.xyxy[0].int().cpu().numpy()\n",
    "\n",
    "            if is_center_in_roi(coord_box, roi):\n",
    "                x1, y1, x2, y2 = coord_box\n",
    "                y1, y2 = max(0, y1), min(image_height, y2)\n",
    "                x1, x2 = max(0, x1), min(image_width, x2)\n",
    "                if y1 >= y2 or x1 >= x2: continue\n",
    "\n",
    "                profundidades_box = depth_frame[y1:y2, x1:x2]\n",
    "                if profundidades_box.size == 0: continue\n",
    "\n",
    "                obj_depth_rel = float(np.median(profundidades_box)) if depth_mode == 'median' else float(np.mean(profundidades_box))\n",
    "                posicao_horizontal = get_horizontal_position(coord_box, image_width)\n",
    "                current_track_id = track_ids[i]\n",
    "\n",
    "                obj_data = {\n",
    "                    'nome': nome,\n",
    "                    'profundidade_rel': obj_depth_rel,\n",
    "                    'bbox': [int(c) for c in coord_box],\n",
    "                    'posicao': posicao_horizontal,\n",
    "                    'track_id': current_track_id\n",
    "                }\n",
    "                detected_objects.append(obj_data)\n",
    "\n",
    "    # Não reordena aqui, a ordenação será feita após o filtro temporal\n",
    "    return detected_objects\n",
    "\n",
    "def metric_proximity_label(distance_m):\n",
    "    \"\"\"Classifica a distância em metros em faixas de proximidade.\"\"\"\n",
    "    if distance_m < 1.0: return \"Próximo\"\n",
    "    elif distance_m <= 2.5: return \"Médio\"\n",
    "    else: return \"Longe\"\n",
    "\n",
    "def gerar_comando_navegacao(objetos_filtrados: list) -> Tuple[str, dict | None]:\n",
    "    \"\"\"\n",
    "    Gera comando de navegação baseado no objeto filtrado mais próximo.\n",
    "    Retorna o comando e os dados do objeto que o gerou (ou None).\n",
    "    \"\"\"\n",
    "    if not objetos_filtrados: return \"Siga em frente.\", None\n",
    "    \n",
    "    obj_mais_proximo = objetos_filtrados[0]\n",
    "    dist_m = obj_mais_proximo.get('distancia_m', 999)\n",
    "    pos = obj_mais_proximo['posicao']\n",
    "    prox_label = metric_proximity_label(dist_m)\n",
    "\n",
    "    comando = \"Siga em frente.\"\n",
    "    objeto_causador = None\n",
    "\n",
    "    if prox_label == \"Próximo\":\n",
    "        if pos == \"Centro\": comando = \"Pare!\"\n",
    "        elif pos == \"Esquerda\": comando = \"Desvie à Direita.\"\n",
    "        elif pos == \"Direita\": comando = \"Desvie à Esquerda.\"\n",
    "        objeto_causador = obj_mais_proximo # Associa o objeto ao comando de alerta\n",
    "        \n",
    "    return comando, objeto_causador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realtime_video_markdown",
   "metadata": {},
   "source": [
    "### Pipeline Principal: Captura e Processamento em Tempo Real\n",
    "\n",
    "Integra todas as funcionalidades: captura de vídeo, MiDaS, YOLO com tracking, filtro SMA, conversão métrica, lógica de navegação e TTS com rate limiting.\n",
    "\n",
    "**Instruções de Uso:**\n",
    "- Execute a célula abaixo para iniciar a câmera e o processamento\n",
    "- Para parar o sistema:\n",
    "  1. **Clique em uma das janelas OpenCV** (`Camera Feed`, `Tracked Feed`, ou `Depth Map`)\n",
    "  2. **Pressione a tecla 'q'** no seu teclado\n",
    "- O sistema fornecerá feedback auditivo quando detectar obstáculos próximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realtime_video_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câmera inicializada. Pressione 'q' para sair.\n",
      "TTS Gerado: 'Pare! bench a 1.0 metros em centro.'\n",
      "TTS Gerado: 'Desvie à Esquerda. chair a 0.5 metros em direita.'\n",
      "TTS Gerado: 'Pare! chair a 1.0 metros em centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.6 metros em centro.'\n",
      "TTS Gerado: 'Desvie à Direita. chair a 0.7 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Direita. bench a 0.6 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Esquerda. bench a 1.0 metros em direita.'\n",
      "TTS Gerado: 'Desvie à Esquerda. chair a 0.5 metros em direita.'\n",
      "TTS Gerado: 'Pare! chair a 0.5 metros em centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.6 metros em centro.'\n",
      "TTS Gerado: 'Desvie à Direita. chair a 0.5 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Esquerda. person a 0.9 metros em direita.'\n",
      "TTS Gerado: 'Desvie à Direita. bench a 0.9 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Direita. chair a 0.5 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Esquerda. chair a 0.5 metros em direita.'\n",
      "TTS Gerado: 'Desvie à Direita. bench a 0.8 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Direita. bench a 0.9 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Direita. chair a 0.7 metros em esquerda.'\n",
      "TTS Gerado: 'Pare! car a 0.5 metros em centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.7 metros em centro.'\n",
      "TTS Gerado: 'Desvie à Direita. chair a 0.6 metros em esquerda.'\n",
      "TTS Gerado: 'Pare! chair a 0.7 metros em centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.4 metros em centro.'\n",
      "TTS Gerado: 'Desvie à Esquerda. chair a 0.5 metros em direita.'\n",
      "TTS Gerado: 'Pare! chair a 1.0 metros em centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.5 metros em centro.'\n",
      "TTS Gerado: 'Desvie à Direita. chair a 0.5 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Esquerda. chair a 0.4 metros em direita.'\n",
      "TTS Gerado: 'Pare! chair a 0.9 metros em centro.'\n",
      "TTS Gerado: 'Pare! chair a 0.5 metros em centro.'\n",
      "TTS Gerado: 'Desvie à Direita. bench a 0.5 metros em esquerda.'\n",
      "TTS Gerado: 'Desvie à Direita. bench a 0.5 metros em esquerda.'\n",
      "TTS Gerado: 'Pare! person a 0.6 metros em centro.'\n",
      "Captura de vídeo encerrada e recursos liberados.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m input_batch = midas_transform(frame_rgb).to(processing_device)\n\u001b[32m     85\u001b[39m output_depth = get_depth(midas_model, input_batch, frame.shape)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m objects_result = \u001b[43myolo_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracker\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbytetrack.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m altura, largura = frame.shape[:\u001b[32m2\u001b[39m]\n\u001b[32m     89\u001b[39m roi = (largura // \u001b[32m6\u001b[39m, \u001b[32m0\u001b[39m, (largura * \u001b[32m5\u001b[39m) // \u001b[32m6\u001b[39m, altura)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\engine\\model.py:601\u001b[39m, in \u001b[36mModel.track\u001b[39m\u001b[34m(self, source, stream, persist, **kwargs)\u001b[39m\n\u001b[32m    599\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[32m    600\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrack\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\engine\\model.py:557\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:230\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:337\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    339\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:185\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    180\u001b[39m visualize = (\n\u001b[32m    181\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    184\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:663\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:139\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:157\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:180\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    181\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:125\u001b[39m, in \u001b[36mDetect.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:  \u001b[38;5;66;03m# Training path\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.export \u001b[38;5;28;01melse\u001b[39;00m (y, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:170\u001b[39m, in \u001b[36mDetect._inference\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28mself\u001b[39m.shape = shape\n\u001b[32m    169\u001b[39m box, \u001b[38;5;28mcls\u001b[39m = x_cat.split((\u001b[38;5;28mself\u001b[39m.reg_max * \u001b[32m4\u001b[39m, \u001b[38;5;28mself\u001b[39m.nc), \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m dbox = \u001b[38;5;28mself\u001b[39m.decode_bboxes(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdfl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.anchors.unsqueeze(\u001b[32m0\u001b[39m)) * \u001b[38;5;28mself\u001b[39m.strides\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat((dbox, \u001b[38;5;28mcls\u001b[39m.sigmoid()), \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gabri\\Documents\\2025\\2sem\\MC949_Visao_Computacional\\venv-T3\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:81\u001b[39m, in \u001b[36mDFL.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Apply the DFL module to input tensor and return transformed output.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m b, _, a = x.shape  \u001b[38;5;66;03m# batch, channels, anchors\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.conv(\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m).view(b, \u001b[32m4\u001b[39m, a)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Imports para TTS e áudio\n",
    "from gtts import gTTS\n",
    "import tempfile\n",
    "import os\n",
    "import playsound\n",
    "import threading # Para tocar áudio sem bloquear o loop principal\n",
    "\n",
    "# Inicializar a captura de vídeo\n",
    "cap = cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    print(\"Erro: Não foi possível abrir a câmera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Câmera inicializada. Pressione 'q' para sair.\")\n",
    "\n",
    "# -- Configurações e Variáveis de Estado --\n",
    "prev_frame_time = time.time()\n",
    "last_tts_time = 0.0\n",
    "TTS_RATE_LIMIT_SECONDS = 6.0 # Intervalo mínimo entre TTS\n",
    "SMA_WINDOW_SIZE = 10\n",
    "depth_history = {}\n",
    "last_seen_frame = {}\n",
    "frame_count = 0\n",
    "fps_list = deque(maxlen=20)\n",
    "\n",
    "# -- Constante de Calibração --\n",
    "# 1. Coloque um objeto a uma distância conhecida (ex: 1.0 metro).\n",
    "# 2. Rode o código e observe a profundidade RELATIVA FILTRADA (filtered_rel_depth) para o track_id do objeto.\n",
    "# C = distancia_real_metros * filtered_rel_depth.\n",
    "CALIBRATION_CONSTANT_C = 0.3 \n",
    "\n",
    "def relative_to_metric(relative_depth, calibration_constant):\n",
    "    \"\"\"Converte profundidade relativa (0-1) para estimativa de distância em metros.\"\"\"\n",
    "    return calibration_constant / (relative_depth + 1e-6)\n",
    "\n",
    "def play_audio_async(file_path):\n",
    "    \"\"\"Toca um arquivo de áudio em uma thread separada.\n",
    "    Se já houver um áudio sendo reproduzido, aguarda até terminar antes de iniciar este.\n",
    "    Remove o arquivo temporário ao final.\n",
    "    \"\"\"\n",
    "    # Lock para garantir reprodução serial (evita sobreposição de áudios)\n",
    "    audio_playback_lock = threading.Lock()\n",
    "    def target():\n",
    "        try:\n",
    "            if audio_playback_lock.locked():\n",
    "                print(f\"Áudio já em reprodução. Aguardando para tocar: {file_path}\")\n",
    "            # Espera até conseguir o lock (bloqueia só esta thread, não o loop principal)\n",
    "            audio_playback_lock.acquire()\n",
    "            try:\n",
    "                if USE_PLAYSOUND:\n",
    "                    try:\n",
    "                        playsound.playsound(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erro no playsound ou módulo ausente: {e}. Simulando reprodução.\")\n",
    "                        print(f\"(Simulando áudio): Tocar {file_path}\")\n",
    "                else:\n",
    "                    print(f\"(Simulando áudio - playsound não disponível): Tocar {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao tocar áudio '{file_path}': {e}\")\n",
    "            finally:\n",
    "                # Pequena pausa antes de remover para garantir flush do player\n",
    "                time.sleep(0.5)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao remover arquivo de áudio temporário '{file_path}': {e}\")\n",
    "                # libera o lock para permitir próxima reprodução\n",
    "                audio_playback_lock.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na thread de áudio para '{file_path}': {e}\")\n",
    "\n",
    "    thread = threading.Thread(target=target)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "\n",
    "# --- Loop Principal ---\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        new_frame_time = time.time()\n",
    "        if not ret: print(\"Erro: Não foi possível ler o frame.\"); break\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        input_batch = midas_transform(frame_rgb).to(processing_device)\n",
    "        output_depth = get_depth(midas_model, input_batch, frame.shape)\n",
    "        objects_result = yolo_model.track(frame_rgb, persist=True, tracker='bytetrack.yaml', verbose=False)\n",
    "\n",
    "        altura, largura = frame.shape[:2]\n",
    "        roi = (largura // 6, 0, (largura * 5) // 6, altura)\n",
    "        \n",
    "        # Obtém objetos detectados com profundidade relativa e track_id\n",
    "        objetos_detectados = get_object_depth_dict(objects_result[0], output_depth, frame.shape, depth_mode='median', roi=roi)\n",
    "\n",
    "        # -- Filtro Temporal e Conversão Métrica --\n",
    "        frame_count += 1\n",
    "        filtered_relative_depths = {}\n",
    "        metric_depths = {}\n",
    "        current_track_ids = set()\n",
    "\n",
    "        for obj in objetos_detectados:\n",
    "            track_id = obj.get('track_id', -1)\n",
    "            if track_id != -1:\n",
    "                current_track_ids.add(track_id)\n",
    "                last_seen_frame[track_id] = frame_count\n",
    "                current_rel_depth = obj['profundidade_rel']\n",
    "                \n",
    "                history = depth_history.setdefault(track_id, deque(maxlen=SMA_WINDOW_SIZE))\n",
    "                history.append(current_rel_depth)\n",
    "                \n",
    "                filtered_rel_depth = float(np.mean(history))\n",
    "                filtered_relative_depths[track_id] = filtered_rel_depth\n",
    "                metric_depths[track_id] = relative_to_metric(filtered_rel_depth, CALIBRATION_CONSTANT_C)\n",
    "\n",
    "        # -- Limpeza de IDs Antigos --\n",
    "        if frame_count % 10 == 0:\n",
    "            ids_to_remove = [tid for tid, last_seen in last_seen_frame.items() if frame_count - last_seen > SMA_WINDOW_SIZE * 3] # Aumentado limiar de limpeza\n",
    "            for tid in ids_to_remove:\n",
    "                depth_history.pop(tid, None)\n",
    "                last_seen_frame.pop(tid, None)\n",
    "\n",
    "        # -- Preparar Lista Final de Objetos para Navegação --\n",
    "        objetos_para_navegacao = []\n",
    "        for obj in objetos_detectados:\n",
    "            track_id = obj.get('track_id', -1)\n",
    "            if track_id != -1 and track_id in metric_depths:\n",
    "                obj_final = obj.copy()\n",
    "                obj_final['distancia_m'] = metric_depths[track_id]\n",
    "                obj_final.pop('profundidade_rel', None)\n",
    "                objetos_para_navegacao.append(obj_final)\n",
    "            elif track_id == -1: # Fallback para objetos sem tracking\n",
    "                 obj_final = obj.copy()\n",
    "                 obj_final['distancia_m'] = relative_to_metric(obj['profundidade_rel'], CALIBRATION_CONSTANT_C)\n",
    "                 obj_final.pop('profundidade_rel', None)\n",
    "                 objetos_para_navegacao.append(obj_final)\n",
    "\n",
    "        objetos_para_navegacao.sort(key=lambda x: x.get('distancia_m', 999))\n",
    "\n",
    "        # -- Geração de Comando e Feedback --\n",
    "        comando, objeto_causador = gerar_comando_navegacao(objetos_para_navegacao)\n",
    "        elapsed_time = new_frame_time - prev_frame_time\n",
    "        fps_list.append(elapsed_time)\n",
    "        avg_elapsed_time = np.mean(fps_list)\n",
    "        fps = 1.0 / avg_elapsed_time if avg_elapsed_time > 0 else 0\n",
    "        prev_frame_time = new_frame_time\n",
    "        #print(f\"FPS: {fps:.1f} | Comando: {comando}\") # Movido para exibição no frame\n",
    "\n",
    "        # -- Visualização --\n",
    "        depth_visual = (output_depth * 255).astype(np.uint8)\n",
    "        depth_colored = cv2.applyColorMap(depth_visual, cv2.COLORMAP_JET)\n",
    "        frame_tracked_plot = objects_result[0].plot() \n",
    "        frame_tracked_bgr = cv2.cvtColor(frame_tracked_plot, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        cv2.rectangle(frame, roi[:2], roi[2:], (0, 255, 0), 2)\n",
    "        cv2.rectangle(frame_tracked_bgr, roi[:2], roi[2:], (0, 255, 0), 2)\n",
    "        fps_text = f\"FPS: {fps:.1f}\"\n",
    "        distancia_text = \"--\" \n",
    "        if objeto_causador: # Usa o objeto que causou o comando\n",
    "            dist_m = objeto_causador.get('distancia_m', 0)\n",
    "            distancia_text = f\"Dist: {dist_m:.2f}m ({objeto_causador['nome']})\"\n",
    "        elif objetos_para_navegacao: # Se comando é 'Siga', mostra o mais próximo\n",
    "            dist_m = objetos_para_navegacao[0].get('distancia_m', 0)\n",
    "            distancia_text = f\"Dist: {dist_m:.2f}m ({objetos_para_navegacao[0]['nome']})\"\n",
    "            \n",
    "        cv2.putText(frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, comando, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, distancia_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_tracked_bgr, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_tracked_bgr, comando, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_tracked_bgr, distancia_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"Camera Feed\", frame)\n",
    "        cv2.imshow(\"Tracked Feed\", frame_tracked_bgr)\n",
    "        cv2.imshow(\"Depth Map\", depth_colored)\n",
    "\n",
    "        # -- Lógica TTS com Rate Limiting e Detalhes --\n",
    "        current_time = time.time()\n",
    "        if comando != \"Siga em frente.\" and (current_time - last_tts_time >= TTS_RATE_LIMIT_SECONDS):\n",
    "            sentence = comando # Começa com o comando principal\n",
    "            if objeto_causador:\n",
    "                nome = objeto_causador['nome']\n",
    "                dist_m = objeto_causador['distancia_m']\n",
    "                pos = objeto_causador['posicao'].lower()\n",
    "                # Monta frase mais detalhada para o alerta\n",
    "                sentence = f\"{comando} {nome} a {dist_m:.1f} metros em {pos}.\"\n",
    "            \n",
    "            print(f\"TTS Gerado: '{sentence}'\") # Log do que será falado\n",
    "            try:\n",
    "                tts = gTTS(sentence, lang='pt')\n",
    "                # Usar um gerenciador de contexto para garantir fechamento\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmpf:\n",
    "                    temp_filename = tmpf.name\n",
    "                tts.save(temp_filename)\n",
    "                last_tts_time = current_time # Atualiza o timestamp\n",
    "                play_audio_async(temp_filename) # Toca em thread separada\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao gerar ou tocar TTS: {e}\")\n",
    "\n",
    "        # -- Saída --\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # --- Finalização Segura --- \n",
    "    # Garante que a câmera seja liberada e as janelas fechadas mesmo se ocorrer um erro no loop\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Captura de vídeo encerrada e recursos liberados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tts_description_markdown",
   "metadata": {},
   "source": [
    "### Funções de Classificação de Proximidade e Geração de Áudio (TTS)\n",
    "\n",
    "Define as faixas de distância métrica e a lógica para gerar os comandos de voz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tts_functions_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_proximity_label(distance_m):\n",
    "    \"\"\"\n",
    "    Classifica a distância em metros em faixas de proximidade.\n",
    "    \n",
    "    Args:\n",
    "        distance_m (float): Distância estimada em metros.\n",
    "    \n",
    "    Returns:\n",
    "        str: 'Próximo' (< 1m), 'Médio' (1-2.5m), ou 'Longe' (> 2.5m).\n",
    "    \"\"\"\n",
    "    if distance_m < 1.0:\n",
    "        return \"Próximo\"\n",
    "    elif distance_m <= 2.5:\n",
    "        return \"Médio\"\n",
    "    else:\n",
    "        return \"Longe\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-T3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
